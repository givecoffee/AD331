{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Handwritten Digit Classification with Feedforward Neural Network\n",
    "\n",
    "**Objective:** Build and train a simple feedforward neural network to classify handwritten digits (0-9) from the MNIST dataset.\n",
    "\n",
    "**Framework:** TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Import Libraries\n",
    "\n",
    "We'll use TensorFlow (Keras API) for building the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Loading and Preprocessing\n",
    "\n",
    "### What we're doing:\n",
    "1. **Load MNIST**: 60,000 training images and 10,000 test images\n",
    "2. **Normalize**: Scale pixel values from [0, 255] to [0, 1]\n",
    "3. **Flatten**: Convert 28×28 images to 784-element vectors\n",
    "4. **One-hot encode**: Convert labels (0-9) to one-hot vectors (e.g., 3 → [0,0,0,1,0,0,0,0,0,0])\n",
    "\n",
    "### Why?\n",
    "- Normalized values prevent numerical instability in gradients\n",
    "- Flattened vectors match the input layer size (784 neurons)\n",
    "- One-hot encoding works with Softmax + Cross-Entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: DATA LOADING\n",
    "# ============================================\n",
    "\n",
    "print(\"Loading MNIST dataset...\")\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(f\"\\nOriginal dataset shapes:\")\n",
    "print(f\"  Training images: {x_train.shape}\")  # (60000, 28, 28)\n",
    "print(f\"  Training labels: {y_train.shape}\")  # (60000,)\n",
    "print(f\"  Test images: {x_test.shape}\")       # (10000, 28, 28)\n",
    "print(f\"  Test labels: {y_test.shape}\")       # (10000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: NORMALIZE PIXEL VALUES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nNormalizing pixel values...\")\n",
    "print(f\"  Original pixel range: [{x_train.min()}, {x_train.max()}]\")\n",
    "\n",
    "# Convert to float and divide by 255\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"  Normalized pixel range: [{x_train.min()}, {x_train.max()}]\")\n",
    "print(\"  ✓ Pixels now in range [0, 1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: FLATTEN IMAGES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nFlattening images from 2D to 1D...\")\n",
    "print(f\"  Before: {x_train.shape}  (28×28 = 784 pixels per image)\")\n",
    "\n",
    "x_train = x_train.reshape(-1, 28 * 28)  # -1 means \"infer this dimension\"\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "\n",
    "print(f\"  After: {x_train.shape}   (784 features per image)\")\n",
    "print(\"  ✓ Ready for neural network input layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: ONE-HOT ENCODE LABELS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nOne-hot encoding labels...\")\n",
    "print(f\"  Original labels (first 10): {y_train[:10]}\")\n",
    "print(f\"  These are integers 0-9\")\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)  # 10 classes (digits 0-9)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print(f\"\\n  One-hot encoded labels shape: {y_train.shape}\")\n",
    "print(f\"  Example: digit 3 is encoded as:\")\n",
    "print(f\"  {y_train[3]}\")\n",
    "print(\"  ✓ Ready for Softmax output layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATA PREPARATION SUMMARY\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA PREPARATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set:\")\n",
    "print(f\"  Images: {x_train.shape}  (60,000 samples of 784 features)\")\n",
    "print(f\"  Labels: {y_train.shape}  (60,000 samples of 10 classes)\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Images: {x_test.shape}   (10,000 samples of 784 features)\")\n",
    "print(f\"  Labels: {y_test.shape}   (10,000 samples of 10 classes)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Model Architecture\n",
    "\n",
    "### Network Structure:\n",
    "- **Input Layer**: 784 neurons (one per flattened pixel)\n",
    "- **Hidden Layer 1**: 128 neurons with ReLU activation\n",
    "- **Hidden Layer 2**: 64 neurons with ReLU activation\n",
    "- **Output Layer**: 10 neurons with Softmax activation (one per digit 0-9)\n",
    "\n",
    "### Why these choices?\n",
    "- **ReLU** in hidden layers: Introduces non-linearity, allows network to learn complex patterns\n",
    "- **Softmax** in output: Converts outputs to probability distribution (sums to 1)\n",
    "- **128 → 64**: Gradually reduces dimensionality from input to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 5: BUILD NEURAL NETWORK MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"Building neural network model...\\n\")\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # Input layer: 784 features (flattened 28x28 images)\n",
    "    layers.Dense(128, activation='relu', input_shape=(784,), name='hidden_layer_1'),\n",
    "    \n",
    "    # Hidden layer 2: 64 neurons with ReLU\n",
    "    layers.Dense(64, activation='relu', name='hidden_layer_2'),\n",
    "    \n",
    "    # Output layer: 10 neurons (one per digit 0-9) with Softmax\n",
    "    layers.Dense(10, activation='softmax', name='output_layer')\n",
    "])\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Compilation\n",
    "\n",
    "### What we're setting:\n",
    "- **Loss Function**: Categorical Cross-Entropy (for multi-class classification)\n",
    "- **Optimizer**: Adam (adapts learning rate automatically)\n",
    "- **Metrics**: Accuracy (what percentage of predictions are correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 6: COMPILE MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"Compiling model...\\n\")\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # For multi-class classification\n",
    "    optimizer='adam',                  # Adaptive learning rate optimizer\n",
    "    metrics=['accuracy']               # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"✓ Model compiled successfully\")\n",
    "print(\"  Loss function: categorical_crossentropy\")\n",
    "print(\"  Optimizer: Adam\")\n",
    "print(\"  Metrics: accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Model Training\n",
    "\n",
    "### Training Parameters:\n",
    "- **Epochs**: 10 (number of times to iterate through entire training dataset)\n",
    "- **Batch Size**: 128 (number of samples to process before updating weights)\n",
    "- **Validation Split**: 0.1 (use 10% of training data to validate during training)\n",
    "\n",
    "### What's happening:\n",
    "1. Feed training data through network (forward pass)\n",
    "2. Compute loss using cross-entropy\n",
    "3. Compute gradients (how much each weight contributed to error)\n",
    "4. Update weights using Adam optimizer (using gradients)\n",
    "5. Repeat for all batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 7: TRAIN MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"Training model...\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,                    # Training images\n",
    "    y_train,                    # Training labels (one-hot encoded)\n",
    "    epochs=10,                  # Number of passes through entire dataset\n",
    "    batch_size=128,             # Process 128 samples before updating weights\n",
    "    validation_split=0.1,       # Use 10% for validation during training\n",
    "    verbose=1                   # Show progress bar\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Training Visualization\n",
    "\n",
    "Let's plot how the loss and accuracy changed during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE TRAINING HISTORY\n",
    "# ============================================\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', marker='o')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Model Loss Over Time')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Model Accuracy Over Time')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Model Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set (data the model has never seen before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 8: EVALUATE ON TEST SET\n",
    "# ============================================\n",
    "\n",
    "print(\"Evaluating model on test set...\\n\")\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Make Predictions on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# MAKE PREDICTIONS ON TEST SAMPLES\n",
    "# ============================================\n",
    "\n",
    "# Get predictions for first 10 test samples\n",
    "sample_predictions = model.predict(x_test[:10])\n",
    "sample_predictions_classes = np.argmax(sample_predictions, axis=1)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(10):\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    predicted_label = sample_predictions_classes[i]\n",
    "    confidence = sample_predictions[i][predicted_label]\n",
    "    \n",
    "    match = \"✓\" if true_label == predicted_label else \"✗\"\n",
    "    print(f\"Sample {i+1}: True={true_label}, Predicted={predicted_label}, Confidence={confidence:.4f} {match}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE PREDICTIONS ON SAMPLE IMAGES\n",
    "# ============================================\n",
    "\n",
    "# Reshape flattened images back to 28x28 for visualization\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Reshape from 784 back to 28x28\n",
    "    image = x_test[i].reshape(28, 28)\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    predicted_label = sample_predictions_classes[i]\n",
    "    \n",
    "    axes[i].imshow(image, cmap='gray')\n",
    "    \n",
    "    title = f\"True: {true_label}, Pred: {predicted_label}\"\n",
    "    color = 'green' if true_label == predicted_label else 'red'\n",
    "    axes[i].set_title(title, color=color)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Model Architecture:\n",
    "- **Input Layer**: 784 neurons\n",
    "- **Hidden Layer 1**: 128 neurons (ReLU activation)\n",
    "- **Hidden Layer 2**: 64 neurons (ReLU activation)\n",
    "- **Output Layer**: 10 neurons (Softmax activation)\n",
    "\n",
    "### Training Details:\n",
    "- **Loss Function**: Categorical Cross-Entropy\n",
    "- **Optimizer**: Adam\n",
    "- **Epochs**: 10\n",
    "- **Batch Size**: 128\n",
    "\n",
    "### Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "print(f\"\\nThis means the model correctly classified {int(test_accuracy*10000)}/10000 test images\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
